{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMvyle6tFBiluNjbQutO5tk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shu-bee/Pytorch_tutorial/blob/main/Learning_PyTorch_with_Examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90q4IOKqeF9O"
      },
      "source": [
        "numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcRTLlDYeUjC",
        "outputId": "b207fd38-4cc8-4bc3-fc28-9558bbbfb455"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "x = np.linspace(-math.pi,math.pi,2000)\n",
        "y = np.sin(x)\n",
        "\n",
        "a0=np.random.randn()\n",
        "b0=np.random.randn()\n",
        "c0=np.random.randn()\n",
        "d0=np.random.randn()\n",
        "\n",
        "a,b,c,d=a0,b0,c0,d0\n",
        "print(a,b,c,d)\n",
        "\n",
        "learning_rate=1e-6\n",
        "for t in range(2000):\n",
        "  # y = a + b x + c x^2 + d x^3\n",
        "  y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
        "\n",
        "  loss = np.square(y_pred-y).sum()\n",
        "  if t %100==99:\n",
        "    print(t,loss)\n",
        "\n",
        "  grad_y_pred=2.0*(y_pred-y)\n",
        "  grad_a = grad_y_pred.sum()\n",
        "  grad_b = (grad_y_pred * x).sum()\n",
        "  grad_c = (grad_y_pred * x ** 2).sum()\n",
        "  grad_d = (grad_y_pred * x ** 3).sum()\n",
        "\n",
        "  a -= learning_rate*grad_a\n",
        "  b -= learning_rate*grad_b\n",
        "  c -= learning_rate*grad_c\n",
        "  d -= learning_rate*grad_d\n",
        "\n",
        "print(f'Result: y = {a} + {b} x+ {c} x^2+ {d} x^3')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-3.14159265 -3.13844949 -3.13530633 ...  3.13530633  3.13844949\n",
            "  3.14159265]\n",
            "-0.11539367782933685 0.5703215804969494 -2.1952562072086987 0.8758641949461718\n",
            "99 171.15088085974332\n",
            "199 118.0690237648532\n",
            "299 82.41625675076581\n",
            "399 58.4443546724296\n",
            "499 42.31234300421377\n",
            "599 31.446488892979783\n",
            "699 24.12093024744778\n",
            "799 19.177470372297325\n",
            "899 15.838242562099083\n",
            "999 13.580384155349789\n",
            "1099 12.052138368087386\n",
            "1199 11.01665030805799\n",
            "1299 10.314288104685115\n",
            "1399 9.837364033911975\n",
            "1499 9.513161472915115\n",
            "1599 9.292529331800335\n",
            "1699 9.142211228164928\n",
            "1799 9.039681941971859\n",
            "1899 8.96966847501264\n",
            "1999 8.921803925061049\n",
            "Result: y = 0.008053208578007459 + 0.8500940431620712 x+ -0.0013893124856754547 x^2+ -0.0923849396340005 x^3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsPpO0aTozc2"
      },
      "source": [
        "pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68kyOLiJoy_q",
        "outputId": "ade5a047-3035-43f9-c1da-f2b059e0b14b"
      },
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "x=torch.linspace(-math.pi,math.pi,2000,device=device,dtype=dtype)\n",
        "y=torch.sin(x)\n",
        "\n",
        "a=torch.randn((),device=device,dtype=dtype)\n",
        "b=torch.randn((),device=device,dtype=dtype)\n",
        "c=torch.randn((),device=device,dtype=dtype)\n",
        "d=torch.randn((),device=device,dtype=dtype)\n",
        "print(a,b,c,d)\n",
        "\n",
        "learning_rate=1e-6\n",
        "for t in range(2000):\n",
        "  y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
        "\n",
        "  loss=(y_pred-y).pow(2).sum().item()\n",
        "  if t%100==99:\n",
        "    print(t,loss)\n",
        "\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    grad_a = grad_y_pred.sum()\n",
        "    grad_b = (grad_y_pred * x).sum()\n",
        "    grad_c = (grad_y_pred * x ** 2).sum()\n",
        "    grad_d = (grad_y_pred * x ** 3).sum()\n",
        "\n",
        "    # Update weights using gradient descent\n",
        "    a -= learning_rate * grad_a\n",
        "    b -= learning_rate * grad_b\n",
        "    c -= learning_rate * grad_c\n",
        "    d -= learning_rate * grad_d\n",
        "\n",
        "\n",
        "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(-0.7675) tensor(-1.6948) tensor(-0.9568) tensor(0.8470)\n",
            "99 145740.46875\n",
            "199 64340.75390625\n",
            "299 44038.46875\n",
            "399 36171.2734375\n",
            "499 31297.67578125\n",
            "599 27513.42578125\n",
            "699 24373.91015625\n",
            "799 21726.6796875\n",
            "899 19485.634765625\n",
            "999 17586.1953125\n",
            "1099 15975.30859375\n",
            "1199 14608.4130859375\n",
            "1299 13447.87890625\n",
            "1399 12461.8857421875\n",
            "1499 11623.5302734375\n",
            "1599 10910.0546875\n",
            "1699 10302.2158203125\n",
            "1799 9783.736328125\n",
            "1899 9340.8505859375\n",
            "1999 8961.9150390625\n",
            "Result: y = -0.5993348360061646 + -1.668657660484314 x + -0.10055004060268402 x^2 + 0.2658855617046356 x^3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyUjscSZsXga"
      },
      "source": [
        "Autograd\n",
        "\n",
        "we can use automatic differentiation to automate the computation of backward passes in neural networks. The autograd package in PyTorch provides exactly this functionality. When using autograd, the forward pass of your network will define a computational graph; nodes in the graph will be Tensors, and edges will be functions that produce output Tensors from input Tensors. Backpropagating through this graph then allows you to easily compute gradients.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gP5DqS5qsbxc",
        "outputId": "2ee226bb-c220-4338-bd9a-1b41b7d8f260"
      },
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "dtype=torch.float\n",
        "device=torch.device(\"cpu\")\n",
        "\n",
        "x = torch.linspace(-math.pi,math.pi,2000,device=device,dtype=dtype)\n",
        "y = torch.sin(x)\n",
        "\n",
        "a = torch.randn((),device=device,dtype=dtype,requires_grad=True)\n",
        "b = torch.randn((),device=device,dtype=dtype,requires_grad=True)\n",
        "c = torch.randn((),device=device,dtype=dtype,requires_grad=True)\n",
        "d = torch.randn((),device=device,dtype=dtype,requires_grad=True)\n",
        "print(a,b,c,d)\n",
        "\n",
        "learning_rate=1e-6\n",
        "for t in range(2000):\n",
        "  y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
        "\n",
        "  loss = (y_pred-y).pow(2).sum()\n",
        "  if t % 100==99:\n",
        "    print(t,loss.item())\n",
        "\n",
        "  loss.backward()\n",
        "  with torch.no_grad():\n",
        "    a -=learning_rate * a.grad\n",
        "    b -=learning_rate * b.grad\n",
        "    c -=learning_rate * c.grad\n",
        "    d -=learning_rate * d.grad\n",
        "\n",
        "    a.grad=None\n",
        "    b.grad=None\n",
        "    c.grad=None\n",
        "    d.grad=None\n",
        "\n",
        "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(-0.2547, requires_grad=True) tensor(-0.6643, requires_grad=True) tensor(0.8711, requires_grad=True) tensor(-1.7299, requires_grad=True)\n",
            "99 1222.2607421875\n",
            "199 815.7386474609375\n",
            "299 545.6017456054688\n",
            "399 366.0360107421875\n",
            "499 246.63446044921875\n",
            "599 167.210693359375\n",
            "699 114.35916900634766\n",
            "799 79.1761474609375\n",
            "899 55.744850158691406\n",
            "999 40.13320541381836\n",
            "1099 29.726648330688477\n",
            "1199 22.786357879638672\n",
            "1299 18.155330657958984\n",
            "1399 15.063515663146973\n",
            "1499 12.99813461303711\n",
            "1599 11.617648124694824\n",
            "1699 10.694300651550293\n",
            "1799 10.076350212097168\n",
            "1899 9.662487030029297\n",
            "1999 9.38510513305664\n",
            "Result: y = -0.012384394183754921 + 0.8365586996078491 x + 0.0021365159191191196 x^2 + -0.09045965224504471 x^3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FNlyYjF2C3_"
      },
      "source": [
        "nn module\n",
        "\n",
        "The nn package defines a set of Modules, which are roughly equivalent to neural network layers. A Module receives input Tensors and computes output Tensors, but may also hold internal state such as Tensors containing learnable parameters. The nn package also defines a set of useful loss functions that are commonly used when training neural networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3k4fPr82pHP",
        "outputId": "8a8d6bb0-c939-412f-9679-b316d313bc4e"
      },
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "x = torch.linspace(-math.pi, math.pi, 2000)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# tensor (x, x^2, x^3).\n",
        "p = torch.tensor([1, 2, 3])\n",
        "xx = x.unsqueeze(-1).pow(p)\n",
        "print(xx)\n",
        "\n",
        "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
        "# is a Module which contains other Modules, and applies them in sequence to\n",
        "# produce its output. The Linear Module computes output from input using a\n",
        "# linear function, and holds internal Tensors for its weight and bias.\n",
        "# The Flatten layer flatens the output of the linear layer to a 1D tensor,\n",
        "# to match the shape of `y`.\n",
        "model=torch.nn.Sequential(\n",
        "    torch.nn.Linear(3,1),\n",
        "    torch.nn.Flatten(0,1)\n",
        ")\n",
        "print(\"model\",model)\n",
        "\n",
        "loss_fn=torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "learning_rate=1e-6\n",
        "for t in range(2000):\n",
        "  y_pred=model(xx)\n",
        "\n",
        "  loss=loss_fn(y_pred,y)\n",
        "  if t % 100 == 99:\n",
        "    print(t,loss.item())\n",
        "\n",
        "  model.zero_grad()\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for param in model.parameters():\n",
        "      param -= learning_rate*param.grad\n",
        "\n",
        "linear_layer=model[0]\n",
        "\n",
        "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ -3.1416,   9.8696, -31.0063],\n",
            "        [ -3.1384,   9.8499, -30.9133],\n",
            "        [ -3.1353,   9.8301, -30.8205],\n",
            "        ...,\n",
            "        [  3.1353,   9.8301,  30.8205],\n",
            "        [  3.1384,   9.8499,  30.9133],\n",
            "        [  3.1416,   9.8696,  31.0063]])\n",
            "model Sequential(\n",
            "  (0): Linear(in_features=3, out_features=1, bias=True)\n",
            "  (1): Flatten(start_dim=0, end_dim=1)\n",
            ")\n",
            "99 677.791259765625\n",
            "199 459.5148010253906\n",
            "299 312.7415771484375\n",
            "399 213.96286010742188\n",
            "499 147.42469787597656\n",
            "599 102.56243896484375\n",
            "699 72.28584289550781\n",
            "799 51.83306121826172\n",
            "899 38.002479553222656\n",
            "999 28.64029884338379\n",
            "1099 22.296289443969727\n",
            "1199 17.992839813232422\n",
            "1299 15.070423126220703\n",
            "1399 13.083646774291992\n",
            "1499 11.731431007385254\n",
            "1599 10.810088157653809\n",
            "1699 10.181598663330078\n",
            "1799 9.752374649047852\n",
            "1899 9.45890998840332\n",
            "1999 9.258031845092773\n",
            "Result: y = 0.016797447577118874 + 0.8433757424354553 x + -0.002897839527577162 x^2 + -0.09142931550741196 x^3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyUV41cbDjG_"
      },
      "source": [
        "nn with optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxbUHf8BDeZO",
        "outputId": "78abf337-2787-46cd-f819-5d6ab4e1bd7b"
      },
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "x = torch.linspace(-math.pi, math.pi, 2000)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# tensor (x, x^2, x^3).\n",
        "p = torch.tensor([1, 2, 3])\n",
        "xx = x.unsqueeze(-1).pow(p)\n",
        "print(xx)\n",
        "\n",
        "model=torch.nn.Sequential(\n",
        "    torch.nn.Linear(3,1),\n",
        "    torch.nn.Flatten(0,1)\n",
        ")\n",
        "print(\"model\",model)\n",
        "\n",
        "loss_fn=torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "learning_rate=1e-3\n",
        "# Use the optim package to define an Optimizer that will update the weights of\n",
        "# the model for us. Here we will use RMSprop; the optim package contains many other\n",
        "# optimization algorithms. The first argument to the RMSprop constructor tells the\n",
        "# optimizer which Tensors it should update.\n",
        "optimizer=torch.optim.RMSprop(model.parameters(),lr=learning_rate)\n",
        "for t in range(2000):\n",
        "  y_pred=model(xx)\n",
        "\n",
        "  loss=loss_fn(y_pred,y)\n",
        "  if t % 100 == 99:\n",
        "    print(t,loss.item())\n",
        "\n",
        "  # Before the backward pass, use the optimizer object to zero all of the\n",
        "  # gradients for the variables it will update (which are the learnable\n",
        "  # weights of the model). This is because by default, gradients are\n",
        "  # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
        "  # is called. Checkout docs of torch.autograd.backward for more details.\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  # Calling the step function on an Optimizer makes an update to its parameters\n",
        "  optimizer.step()\n",
        "\n",
        "linear_layer=model[0]\n",
        "\n",
        "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ -3.1416,   9.8696, -31.0063],\n",
            "        [ -3.1384,   9.8499, -30.9133],\n",
            "        [ -3.1353,   9.8301, -30.8205],\n",
            "        ...,\n",
            "        [  3.1353,   9.8301,  30.8205],\n",
            "        [  3.1384,   9.8499,  30.9133],\n",
            "        [  3.1416,   9.8696,  31.0063]])\n",
            "model Sequential(\n",
            "  (0): Linear(in_features=3, out_features=1, bias=True)\n",
            "  (1): Flatten(start_dim=0, end_dim=1)\n",
            ")\n",
            "99 1959.2606201171875\n",
            "199 1195.62548828125\n",
            "299 1020.6521606445312\n",
            "399 853.280517578125\n",
            "499 692.38916015625\n",
            "599 547.0382080078125\n",
            "699 420.3686828613281\n",
            "799 312.79595947265625\n",
            "899 223.8001251220703\n",
            "999 152.2157745361328\n",
            "1099 97.26840209960938\n",
            "1199 57.58738327026367\n",
            "1299 31.659496307373047\n",
            "1399 17.07789421081543\n",
            "1499 10.783099174499512\n",
            "1599 9.151917457580566\n",
            "1699 8.966609954833984\n",
            "1799 8.906667709350586\n",
            "1899 8.904130935668945\n",
            "1999 8.9237060546875\n",
            "Result: y = 0.00049802684225142 + 0.8562657237052917 x + 0.0004980287048965693 x^2 + -0.09384801238775253 x^3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1QjLwMCmbPQ"
      },
      "source": [
        "Custum nn Modules\n",
        "\n",
        "Sometimes you will want to specify models that are more complex than a sequence of existing Modules; for these cases you can define your own Modules by subclassing nn.Module and defining a forward which receives input Tensors and produces output Tensors using other modules or other autograd operations on Tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8buSReimfzn",
        "outputId": "ad1dd600-e8c5-43e3-f770-fe16aee2792f"
      },
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "class Polynomial3(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    In the constructor we instantiate four parameters and assign them as\n",
        "    member parameters.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.a=torch.nn.Parameter(torch.randn(()))\n",
        "    self.b=torch.nn.Parameter(torch.randn(()))\n",
        "    self.c=torch.nn.Parameter(torch.randn(()))\n",
        "    self.d=torch.nn.Parameter(torch.randn(()))\n",
        "  \n",
        "  def forward(self,x):\n",
        "    \"\"\"\n",
        "    In the forward function we accept a Tensor of input data and we must return\n",
        "    a Tensor of output data. We can use Modules defined in the constructor as\n",
        "    well as arbitrary operators on Tensors.\n",
        "    \"\"\"\n",
        "    return self.a+self.b*x+self.c*x**2+self.d*x**3\n",
        "\n",
        "  def string(self):\n",
        "    \"\"\"\n",
        "    Just like any class in Python, you can also define custom method on PyTorch modules\n",
        "    \"\"\"\n",
        "    return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'\n",
        "\n",
        "#Create Tensors to hold input and outputs.\n",
        "x=torch.linspace(-math.pi,math.pi,2000)\n",
        "y=torch.sin(x)\n",
        "\n",
        "#Construct our model by instantiating the class defined above \n",
        "model = Polynomial3()\n",
        "\n",
        "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters of the nn.Linear\n",
        "# module which is members of the model.\n",
        "criterion=torch.nn.MSELoss(reduction='sum')\n",
        "optimizer=torch.optim.SGD(model.parameters(),lr=1e-6)\n",
        "for t in range(2000):\n",
        "  y_pred=model(x)\n",
        "\n",
        "  loss=criterion(y_pred,y)\n",
        "  if t % 100 ==99:\n",
        "    print(t,loss.item())\n",
        "    \n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  \n",
        "print(f'Result:{model.string()}')\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99 1556.3314208984375\n",
            "199 1101.218505859375\n",
            "299 780.07177734375\n",
            "399 553.4161987304688\n",
            "499 393.4232482910156\n",
            "599 280.4683837890625\n",
            "699 200.71055603027344\n",
            "799 144.3852996826172\n",
            "899 104.60313415527344\n",
            "999 76.50167083740234\n",
            "1099 56.649078369140625\n",
            "1199 42.62240219116211\n",
            "1299 32.71099853515625\n",
            "1399 25.70684242248535\n",
            "1499 20.756732940673828\n",
            "1599 17.257986068725586\n",
            "1699 14.784891128540039\n",
            "1799 13.036649703979492\n",
            "1899 11.800713539123535\n",
            "1999 10.92690658569336\n",
            "Result:y=-0.048358283936977386+0.8521280884742737+0.008342606946825981+-0.09267426282167435\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go3QI2xE2TXR"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZfzexs42Yn_",
        "outputId": "959dcf22-f76f-46d5-8da6-5083c24a9584"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import torch\n",
        "import math\n",
        "\n",
        "\n",
        "class Polynomial3(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate four parameters and assign them as\n",
        "        member parameters.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.a = torch.nn.Parameter(torch.randn(()))\n",
        "        self.b = torch.nn.Parameter(torch.randn(()))\n",
        "        self.c = torch.nn.Parameter(torch.randn(()))\n",
        "        self.d = torch.nn.Parameter(torch.randn(()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Tensor of input data and we must return\n",
        "        a Tensor of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Tensors.\n",
        "        \"\"\"\n",
        "        return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
        "\n",
        "    def string(self):\n",
        "        \"\"\"\n",
        "        Just like any class in Python, you can also define custom method on PyTorch modules\n",
        "        \"\"\"\n",
        "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'\n",
        "\n",
        "\n",
        "# Create Tensors to hold input and outputs.\n",
        "x = torch.linspace(-math.pi, math.pi, 2000)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# Construct our model by instantiating the class defined above\n",
        "model = Polynomial3()\n",
        "\n",
        "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters of the nn.Linear\n",
        "# module which is members of the model.\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\n",
        "for t in range(2000):\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(x)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = criterion(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(f'Result: {model.string()}')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99 1010.9580688476562\n",
            "199 678.1954956054688\n",
            "299 456.196044921875\n",
            "399 308.0096740722656\n",
            "499 209.03799438476562\n",
            "599 142.8973846435547\n",
            "699 98.66962432861328\n",
            "799 69.07551574707031\n",
            "899 49.259788513183594\n",
            "999 35.982154846191406\n",
            "1099 27.078868865966797\n",
            "1199 21.104143142700195\n",
            "1299 17.09164047241211\n",
            "1399 14.394590377807617\n",
            "1499 12.580179214477539\n",
            "1599 11.35848617553711\n",
            "1699 10.535148620605469\n",
            "1799 9.979721069335938\n",
            "1899 9.604706764221191\n",
            "1999 9.351224899291992\n",
            "Result: y = -0.01507451944053173 + 0.8390489220619202 x + 0.0026006055995821953 x^2 + -0.09081386774778366 x^3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcJn6OCY3AAQ"
      },
      "source": [
        "Control Flow + Weight Sharing\n",
        "\n",
        "As an example of dynamic graphs and weight sharing, we implement a very strange model: a third-fifth order polynomial that on each forward pass chooses a random number between 3 and 5 and uses that many orders, reusing the same weights multiple times to compute the fourth and fifth order.\n",
        "\n",
        "For this model we can use normal Python flow control to implement the loop, and we can implement weight sharing by simply reusing the same parameter multiple times when defining the forward pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APaxHSFj4rKN",
        "outputId": "0562704d-b574-44d7-8a11-bd5921f8f41b"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import random\n",
        "import torch\n",
        "import math\n",
        "\n",
        "\n",
        "class DynamicNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate four parameters and assign them as\n",
        "        member parameters.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.a = torch.nn.Parameter(torch.randn(()))\n",
        "        self.b = torch.nn.Parameter(torch.randn(()))\n",
        "        self.c = torch.nn.Parameter(torch.randn(()))\n",
        "        self.d = torch.nn.Parameter(torch.randn(()))\n",
        "        self.e = torch.nn.Parameter(torch.randn(()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        For the forward pass of the model, we randomly choose either 4, 5\n",
        "        and reuse the e parameter to compute the contribution of these orders.\n",
        "\n",
        "        Since each forward pass builds a dynamic computation graph, we can use normal\n",
        "        Python control-flow operators like loops or conditional statements when\n",
        "        defining the forward pass of the model.\n",
        "\n",
        "        Here we also see that it is perfectly safe to reuse the same parameter many\n",
        "        times when defining a computational graph.\n",
        "        \"\"\"\n",
        "        return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
        "        for exp in range(4,random.randint(4,6)):\n",
        "          y=y+self.e*x**exp\n",
        "        return y\n",
        "\n",
        "    def string(self):\n",
        "        \"\"\"\n",
        "        Just like any class in Python, you can also define custom method on PyTorch modules\n",
        "        \"\"\"\n",
        "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?'\n",
        "\n",
        "\n",
        "# Create Tensors to hold input and outputs.\n",
        "x = torch.linspace(-math.pi, math.pi, 2000)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# Construct our model by instantiating the class defined above\n",
        "model = DynamicNet()\n",
        "\n",
        "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters of the nn.Linear\n",
        "# module which is members of the model.\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)\n",
        "for t in range(30000):\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(x)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = criterion(y_pred, y)\n",
        "    if t % 2000 == 1999:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(f'Result: {model.string()}')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1999 1905.299072265625\n",
            "3999 846.3731079101562\n",
            "5999 379.2395324707031\n",
            "7999 172.90354919433594\n",
            "9999 81.63180541992188\n",
            "11999 41.1929931640625\n",
            "13999 23.243913650512695\n",
            "15999 15.261205673217773\n",
            "17999 11.703105926513672\n",
            "19999 10.113356590270996\n",
            "21999 9.40114974975586\n",
            "23999 9.0811767578125\n",
            "25999 8.936957359313965\n",
            "27999 8.871721267700195\n",
            "29999 8.842118263244629\n",
            "Result: y = -0.003130276221781969 + 0.8528169393539429 x + 0.0005400248919613659 x^2 + -0.09277224540710449 x^3 + 1.4095135927200317 x^4 ? + 1.4095135927200317 x^5 ?\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}